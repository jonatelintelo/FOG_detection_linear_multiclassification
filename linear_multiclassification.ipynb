{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\nimport random\nimport time\nimport functools\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom scipy import stats\n\nfrom sklearn.metrics import accuracy_score, average_precision_score\n\n# Create some useful global parameters.\nMAIN_DIR = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTARGETS = [\"StartHesitation\", \"Turn\", \"Walking\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create Data Subset**","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    \"\"\"\n    Reduces the memory usage of the passed dataframe.\n    Reference: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65\n    \n    Args:\n        df (pandas.core.frame.DataFrame): The pandas dataframe to be reduced in memory.\n    Returns:\n        pandas.core.frame.DataFrame: The pandas dataframe with memory usage reduced.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    print('\\nStart reducing dataframe memory usage.')\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in tqdm(df.columns):\n        col_type = df[col].dtype.name\n\n        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n            if (col_type != 'object'):\n                c_min = df[col].min()\n                c_max = df[col].max()\n\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n\n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        pass\n            else:\n                df[col] = df[col].astype('category')\n                \n    mem_usg = df.memory_usage().sum() / 1024**2 \n    \n    print(\"Memory usage became: \",mem_usg,\" MB\")\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_target(data, targets_list):\n    \"\"\"\n    Encodes the targets from the target list in the passed data to numbers.\n\n    Args:\n        data (pandas.core.series.Series): Data that needs to be encoded.\n        targets_list (list): List containing the target values to be encoded.\n    Returns:\n        numpy.ndarray: The label encoded array.\n    \"\"\"\n    conditions = []\n\n    for target in targets_list:\n        conditions.append((data[target] == 1))\n\n    event = np.select(conditions, targets_list, default='Normal')\n    le = LabelEncoder()\n\n    return le.fit_transform(event)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data(dataset, datatype, as_dataframe):\n    \"\"\"\n    Based on whether datatype is train or test it returns the train or test\n    data as a numpy array. The following columns in the numpy array are as \n    follows: Id, Time, AccV, AccML, AccAP, StartHesitation, Turn, Walking,\n    Valid, Task, Target.\n    \n    Args:\n        dataset (str):\n        datatype (str):\n        subject_id (str):\n    Returns:\n        numpy.ndarray: The dataset as per the parameters as a numpy array.\n    \"\"\"\n    metadata = pd.read_csv(MAIN_DIR + dataset + \"_metadata.csv\")\n    DATA_ROOT = MAIN_DIR + datatype + \"/\" + dataset\n    \n    # Loop over over the files in the directory and append to the \n    # resulting dataframe.\n    df_res = pd.DataFrame()\n    \n    print(\"\\nStart reading files.\")\n    for root, dirs, files in os.walk(DATA_ROOT):\n        for name in tqdm(files):\n            f = os.path.join(root, name)\n            query_datatype = pd.read_csv(f)\n            query_datatype['File'] = name.replace(\".csv\", \"\")\n            df_res = pd.concat([df_res,query_datatype])\n    \n    # Merge the metadata and accdata together in one dataframe.\n    df_res = metadata.merge(df_res, how='inner', left_on='Id', right_on='File')\n    df_res = df_res.drop(['File','Subject','Visit','Medication'], axis=1)\n\n    # Only if we are creating a training set we encode the features for usage.\n    if datatype == 'train':\n        if dataset == 'tdcsfog':\n            df_res['Valid'] = 1\n            df_res['Task'] = 1\n        df_res['Target'] = encode_target(df_res, TARGETS)\n    \n    if dataset == 'tdcsfog':\n            df_res = df_res.drop(['Test'], axis=1)\n\n    df_res = reduce_memory_usage(df_res)        \n    \n    if as_dataframe:\n        return df_res\n    \n    return df_res.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    def check_ids(data, index, window):\n        \"\"\"\n        Checks if the window around index only contains the same ids.\n        This function was created because numpy is faster for large \n        datasets than Pandas alternatives.\n        \n        Args:\n            index (int): The index to check the window for.\n            window (int): The future and past window size.\n        Returns:\n            bool: True if all values in file_ids are the same, False otherwise.\n        \"\"\"\n        # Put all file IDs of the current window into an array.\n        file_ids = data[index - window : index + window,0]\n        \n        # Return False if no correct window can be made for file_ids.\n        if len(file_ids)==0:\n            return False\n        \n        return (file_ids[0] == file_ids).all()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_windows(data, window_past, window_future, threshold):\n    feature_windows = []\n    target_windows = []\n    t_windows = []\n\n    print(\"\\nStart creating windows.\")\n    for idx, item in enumerate(tqdm(data)):\n        if(\n          (idx - window_past < 0) or\n          (idx + window_future > len(data)) or\n          (np.sum(np.ediff1d(data[idx - window_past : idx + window_future,1])) > ((window_past + window_future) - 1)) or\n          (not check_ids(data, idx, window_past + window_future))\n          ): \n            continue\n        \n        # If we dont want thresholding, we can set threshold to None\n        if threshold == None:\n            target = 4\n        else:\n            candidates = pd.Series(data[idx - window_past : idx + window_future,-1]).value_counts(normalize=True)\n            target = None\n\n            # Check if the largest candidate is above the threshold.\n            if (candidates.to_numpy()[0] >= threshold):\n                target = candidates.index[0]\n                \n        # Match the target and transform to correct binary array instance.\n        # Will only 'continue' if candidates check fails.\n        match target:\n            case 0:\n                targets = [0,0,0]\n            case 1:\n                targets = [1,0,0]\n            case 2:\n                targets = [0,1,0]\n            case 3:\n                targets = [0,0,1]\n            case 4:\n                targets = data[idx, 5:8]\n            case _:\n                continue\n        \n        feature_windows.append(data[idx - window_past : idx + window_future,1:5])\n        target_windows.append(targets)\n        t_windows.append(data[idx, -3]*data[idx, -2])\n\n    return stats.zscore(np.array(feature_windows, dtype='float')), np.array(target_windows, dtype='float'), np.array(t_windows, dtype='float')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_windows(data, window_past, window_future):\n    feature_windows = []\n    _ids = []\n\n    print(\"\\nStart creating test windows.\")\n    for idx, _ in enumerate(tqdm(data)):\n        if(idx - window_past < 0):\n            feature_windows.append(np.concatenate([np.zeros((window_past, 4)), data[idx - window_past : idx + window_future,1:5]], axis=0))\n            _ids.append(data[idx,0])\n        if (idx + window_future > len(data)):\n            feature_windows.append(np.concatenate([data[idx - window_past : idx + window_future,1:5],np.zeros((window_future, 4))], axis=0))\n            _ids.append(data[idx,0])\n        else:\n            feature_windows.append(data[idx - window_past : idx + window_future,1:5])\n            _ids.append(data[idx,0])\n            \n    return stats.zscore(np.array(feature_windows, dtype='float')), _ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_data(dataset, datatype):\n    dataframe = read_data(dataset, datatype, True)\n    ones = dataframe.loc[dataframe[\"Target\"] == 1, :]\n    twos = dataframe.loc[dataframe[\"Target\"] == 2, :]\n    threes = dataframe.loc[dataframe[\"Target\"] == 3, :]\n    zeros = dataframe.loc[dataframe[\"Target\"] == 0, :]\n#     zeros = zeros.loc[zeros[\"Task\"] == 1, :]\n    zeros = zeros.sample(len(ones) + len(twos) + len(threes))\n    sampled_df = pd.concat([ones, twos, threes, zeros], ignore_index=False)\n#     sampled_df = sampled_df.loc[sampled_df[\"Task\"] == 1, :]\n    \n    return sampled_df.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset Class**","metadata":{}},{"cell_type":"code","source":"class FOGDataset(Dataset):\n    \n    def __init__(self, datatype, threshold, window_future, window_past):\n        self.datatype = datatype\n        \n        # We only set the targets variable if we are making a training dataset.\n        # The train features in order are: Id, Time, AccV, AccML, AccAP, \n        # StartHesitation, Turn, Walking, Valid, Task, Target.\n        # The test features in order are: Id, Time, AccV, AccML, AccAP\n        if datatype == \"train\":\n            defog_data = sample_data('defog', datatype)\n            defog_features, defog_targets, defog_t = get_windows(defog_data, window_past, window_future, threshold)\n            \n            tdcsfog_data = sample_data('tdcsfog', datatype)\n            tdcsfog_features, tdcsfog_targets, tdcsfog_t = get_windows(tdcsfog_data, window_past, window_future, threshold)\n            \n            self.features = np.concatenate((defog_features ,tdcsfog_features))\n            self.targets = np.concatenate((defog_targets ,tdcsfog_targets))\n            self.t = np.concatenate((defog_t ,tdcsfog_t))\n            self.length = len(self.features)\n            \n        # If datatype is test we want to pad.         \n        else:\n            defog_data = read_data('defog', 'test', False)\n            defog_features, defog_ids = get_test_windows(defog_data, window_past, window_future)\n            \n            tdcsfog_data = read_data('tdcsfog', 'test', False)             \n            tdcsfog_features, tdcsfog_ids = get_test_windows(tdcsfog_data, window_past, window_future)\n            \n            self.features = np.concatenate((defog_features, tdcsfog_features))\n            self.ids = np.concatenate((defog_ids, tdcsfog_ids))\n            self.length = len(self.features)\n            \n    def __len__(self):\n        \"\"\"\n        Returns:\n            int: The length of the data object of the dataset class.\n        \"\"\"\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Creates the batches for the dataloader.\n        Returns:\n            torch.Tensor: A tensor with the features per batch.\n            torch.Tensor: A tensor with the target for each sample in the batch.\n        \"\"\"\n        # Check if the datatype passed is train, we only create targets if the datatype is train.\n        if self.datatype == \"train\":\n            features = torch.tensor(self.features[idx].astype('float'))\n            targets = torch.tensor(self.targets[idx].astype('float'))\n            t = torch.tensor(self.t[idx].astype('float'))\n            \n            return features, targets, t\n        else:\n            _id = self.ids[idx] + \"_\" + str(idx)\n        # Commented out line goes in pair with the reshape in forward() function to select only n = 8th row of window.\n#             features = self.features[::8, :][::-1, :]\n            features = torch.tensor(self.features[idx].astype('float'))\n    \n            return _id, features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"def _block(in_features, out_features, drop_rate):\n    return nn.Sequential(\n        nn.Linear(in_features, out_features),\n        nn.BatchNorm1d(out_features),\n        nn.ReLU(),\n        nn.Dropout(drop_rate)\n    )\n\nclass FOGModel(nn.Module):\n    def __init__(self, p, dim, nblocks, window_size):\n        super(FOGModel, self).__init__()\n        self.window_size = window_size\n        self.dropout = nn.Dropout(p)\n        # Commented out line goes in pair with the slicing in the __getitem__() function to select only n = 8th row of window.\n#         self.in_layer = nn.Linear(int(window_size / 8) * 3, dim)\n        self.in_layer = nn.Linear(window_size * 4, dim)\n        self.blocks = nn.Sequential(*[_block(dim, dim, p) for _ in range(nblocks)])\n        self.out_layer = nn.Linear(dim, 3)#output dimension, 3 if binary case, I am guessing 1 with non binary case\n        \n    def forward(self, x):\n        # Commented out line goes in pair with the slicing in the __getitem__() function to select only n = 8th row of window.\n#         x = x.view(-1, int(self.window_size / 8) * 3)\n        x = x.view(-1, self.window_size * 4)\n        x = self.in_layer(x)\n        for block in self.blocks:\n            x = block(x)\n        x = self.out_layer(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler\n\ndef train_one_epoch(model, loader, optimizer, criterion):\n    loss_sum = 0.\n    scaler = GradScaler()\n    \n    model.train()\n    for x,y,t in tqdm(loader):\n        x = x.to(DEVICE).float()\n        y = y.to(DEVICE).float()\n        t = t.to(DEVICE).float()\n        \n        y_pred = model(x)\n        loss = criterion(y_pred, y)\n        loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n        \n        t_sum = torch.sum(t)\n        if t_sum > 0:\n            loss = torch.sum(loss)/t_sum\n        else:\n            loss = torch.sum(loss)*0.\n        \n        # loss.backward()\n        scaler.scale(loss).backward()\n        # optimizer.step()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        optimizer.zero_grad()\n        \n        loss_sum += loss.item()\n    \n    print(f\"Train Loss: {(loss_sum/len(loader)):.04f}\")\n    return loss_sum/len(loader)\n\ndef validation_one_epoch(model, loader, criterion):\n    loss_sum = 0.\n    y_true_epoch = []\n    y_pred_epoch = []\n    t_valid_epoch = []\n    \n    model.eval()\n    for x,y,t in tqdm(loader):\n        x = x.to(DEVICE).float()\n        y = y.to(DEVICE).float()\n        t = t.to(DEVICE).float()\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n            \n            t_sum = torch.sum(t)\n            if t_sum > 0:\n                loss = torch.sum(loss)/t_sum\n            else:\n                loss = torch.sum(loss)*0.\n        \n        loss_sum += loss.item()\n        y_true_epoch.append(y.cpu().numpy())\n        y_pred_epoch.append(y_pred.cpu().numpy())\n        t_valid_epoch.append(t.cpu().numpy())\n        \n    y_true_epoch = np.concatenate(y_true_epoch, axis=0)\n    y_pred_epoch = np.concatenate(y_pred_epoch, axis=0)\n    \n    t_valid_epoch = np.concatenate(t_valid_epoch, axis=0)\n    y_true_epoch = y_true_epoch[t_valid_epoch > 0, :]\n    y_pred_epoch = y_pred_epoch[t_valid_epoch > 0, :]\n    \n    scores = [average_precision_score(y_true_epoch[:,i], y_pred_epoch[:,i]) for i in range(3)]\n    mean_score = np.mean(scores)\n    print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}, ClassWise: {scores[0]:.03f},{scores[1]:.03f},{scores[2]:.03f}\")\n    \n    return loss_sum/len(loader), mean_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"markdown","source":"**Initialize the data set for the model**","metadata":{}},{"cell_type":"code","source":"# window_size = 256\ndatatype = \"train\"\nthreshold = None\nwindow_future = 128\nwindow_past = 128\n\nnum_epochs = 15\nmodel_dropout = 0.2\nmodel_hidden = 512\nmodel_nblocks = 3\n\nbatch_size = 1024\nlr = 0.0015\n\nprint(\"Initialising data:\")\ndataset_train = FOGDataset(\n    datatype = datatype,\n    threshold = threshold,\n    window_future = window_future,\n    window_past = window_past\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train and validate the model**","metadata":{}},{"cell_type":"code","source":"model = FOGModel(model_dropout, model_hidden, model_nblocks, window_past + window_future).to(DEVICE)\n# print(model)\nprint(f\"\\nNumber of parameters in model - {count_parameters(model):,}\")\n\ntrain_size = int(0.8 * len(dataset_train))\ntest_size = len(dataset_train) - train_size\ntrain_dataset, valid_dataset = torch.utils.data.random_split(dataset_train, [train_size, test_size])\n\nprint(f\"\\nLength of datasets: train - {len(train_dataset)}, valid - {len(valid_dataset)}\")\n\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers=2, shuffle = True)\nvalid_loader = DataLoader(valid_dataset, batch_size = batch_size, num_workers=2, shuffle = True)\n# Only shuffle on train and validation, do not shuffle on test.\n\noptimizer = torch.optim.Adam(model.parameters(), lr = lr)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction = 'none').to(DEVICE)\n# sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)\n\nprint(\"=\"*50)\nprint(\"Start training:\\n\")\n\nmax_score = 0.0\n\nprint(\"=\"*50)\nfor epoch in range(num_epochs):\n    print(f\"Epoch: {epoch}\")\n    train_loss  = train_one_epoch(model, train_loader, optimizer, criterion)\n    valid_loss, valid_score = validation_one_epoch(model, valid_loader, criterion)\n    # sched.step()\n\n    if valid_score > max_score:\n        max_score = valid_score\n        torch.save(model.state_dict(), \"best_model_state.h5\")\n        print(\"Saving Model ...\")\n\n    print(\"=\"*50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"# ONLY USED WHEN DOING THE LEADERBOARD SUBMISSION\ndatatype = \"test\"\nthreshold = None\nwindow_future = 128\nwindow_past = 128\n\ndataset_test = FOGDataset(\n    datatype = datatype,\n    threshold = threshold,\n    window_future = window_future,\n    window_past = window_past\n)\n\ntest_loader = DataLoader(dataset_test, batch_size = batch_size, num_workers=2, shuffle = False)\n# Shuffle should be False for test set.\n\nmodel.eval()\n\nids = []\npreds = []\n\nfor _id, x in tqdm(test_loader):\n    x = x.to(DEVICE).float()\n    with torch.no_grad():\n        y_pred = model(x)*0.1\n    \n    ids.extend(_id)\n    preds.extend(list(np.nan_to_num(y_pred.cpu().numpy())))\n    \npreds = np.array(preds)\nsubmission = pd.DataFrame({'Id': ids, 'StartHesitation': np.round(preds[:,0],5), \\\n                           'Turn': np.round(preds[:,1],5), 'Walking': np.round(preds[:,2],5)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv\")\nsubmission = pd.merge(sample_submission[['Id']], submission, how='left', on='Id').fillna(0.0)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
